{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LinReg:\n",
      "RMSE Training:9.067573557995281\n",
      "SMAPE Training:0.16707112656491704\n",
      "Validation LinReg:\n",
      "RMSE Training:10.500681345539894\n",
      "SMAPE Training:0.19050978291064477\n",
      "Training LinReg:\n",
      "RMSE Training:8.525004907522687\n",
      "SMAPE Training:0.18054953531836895\n",
      "Validation LinReg:\n",
      "RMSE Training:9.929346359225526\n",
      "SMAPE Training:0.20212645021127695\n",
      "\n",
      "right:183 size:349 %:0.5243553008595988\n",
      "i:1\n",
      "right:146 size:294 %:0.4965986394557823\n",
      "i:2\n",
      "right:120 size:243 %:0.49382716049382713\n",
      "i:3\n",
      "right:103 size:201 %:0.5124378109452736\n",
      "i:4\n",
      "right:87 size:157 %:0.554140127388535\n",
      "i:5\n",
      "right:63 size:119 %:0.5294117647058824\n",
      "i:6\n",
      "right:45 size:89 %:0.5056179775280899\n",
      "i:7\n",
      "right:33 size:64 %:0.515625\n",
      "i:8\n",
      "right:23 size:43 %:0.5348837209302325\n",
      "i:9\n",
      "right:16 size:32 %:0.5\n",
      "i:10\n",
      "right:11 size:23 %:0.4782608695652174\n",
      "i:11\n",
      "right:7 size:17 %:0.4117647058823529\n",
      "i:12\n",
      "right:4 size:11 %:0.36363636363636365\n",
      "i:13\n",
      "right:3 size:9 %:0.3333333333333333\n",
      "\n",
      "Training\n",
      "Class priors 1:0.49283667621776506 0:0.5071633237822349\n",
      "Accuracy: 0.5931232091690545\n",
      "Precison: 0.5882352941176471\n",
      "Recall: 0.5813953488372093\n",
      "FMeasure: 0.5847953216374269\n",
      "\n",
      "Validation\n",
      "Class priors 1:0.4383954154727794 0:0.5616045845272206\n",
      "Accuracy: 0.5243553008595988\n",
      "Precison: 0.4619883040935672\n",
      "Recall: 0.5163398692810458\n",
      "FMeasure: 0.4876543209876544\n",
      "\n",
      "Logistic Regression: 0.5472779369627507\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "def zscore(data,data2):\n",
    "    data[data == 'NA'] = 0\n",
    "    data = data.astype(float)\n",
    "    data2[data2 == 'NA'] = 0\n",
    "    data2 = data2.astype(float)\n",
    "    m = np.mean(data)\n",
    "    s = np.std(data,ddof=1)\n",
    "    return (data-m)/s,(data2-m)/s\n",
    "\n",
    "def onehot(data,name):\n",
    "    values = list(set(data))\n",
    "    arrRet = np.zeros(shape=(len(data),len(values)))\n",
    "    for x in range(0,len(data)):\n",
    "        for y in range(0,len(values)):\n",
    "            arrRet[x,y] = values[y] == data[x]\n",
    "    values = np.char.add(name + \"_\", values)\n",
    "    return values,arrRet\n",
    "\n",
    "def trainingSplit(data):\n",
    "    np.random.shuffle(data)\n",
    "    trainingIndex = int(len(data)/3)\n",
    "    trainingData = data[:-trainingIndex,:]\n",
    "    validationData = data[(len(data) - trainingIndex):,:]\n",
    "    return trainingData,validationData\n",
    "\n",
    "def rmse(y,yHat):\n",
    "\treturn np.sqrt(np.mean((yHat-y)**2))\n",
    "\n",
    "def smape(y,yHat):\n",
    "\treturn np.mean(np.abs(y-yHat)/(np.abs(y) + np.abs(yHat)))\n",
    "\n",
    "def getFieldIndex(header,fld):\n",
    "      return np.where(header == ('\"' + fld + '\"'))[0][0]\n",
    "\n",
    "def diag(X):\n",
    "    xDiag = np.zeros((X.shape[0],X.shape[0]),int)\n",
    "    np.fill_diagonal(xDiag,X)\n",
    "    return xDiag\n",
    "\n",
    "def linregstats(Y,yHat):\n",
    "    print(\"RMSE Training:\" + str(rmse(Y,yHat)))\n",
    "    print(\"SMAPE Training:\" + str(smape(Y,yHat)))\n",
    "\n",
    "def svmstats(yHat,Y):\n",
    "    correct = 0\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    tn = 0\n",
    "    fn = 0\n",
    "    p = 0\n",
    "    n = 0\n",
    "    yAbs = np.abs(yHat)\n",
    "    res = np.hstack((np.where(yHat > 0, 1,-1),np.atleast_2d(Y).T,(np.where(yAbs > .5,1,0))))\n",
    "    #res = res[res[:,2] == 1]\n",
    "    tp = np.sum((res[:,0] == 1) & (res[:,1] == 1))\n",
    "    fp = np.sum((res[:,0] == 1) & (res[:,1] == -1))\n",
    "    tn = np.sum((res[:,0] == -1) & (res[:,1] == 1))\n",
    "    fn = np.sum((res[:,0] == -1) & (res[:,1] == 1))\n",
    "    correct = np.sum(res[:,0] == res[:,1])\n",
    "    n = np.sum(res[:,1] == -1)\n",
    "    p = np.sum(res[:,1] == 1)\n",
    "    precision = tp/(tp+fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    fMeasure = (2*precision*recall)/(precision+recall)\n",
    "    print(f\"Class priors 1:{p/res.shape[0]} 0:{n/res.shape[0]}\")\n",
    "    print(f\"Accuracy: {correct/res.shape[0]}\")\n",
    "    print(f\"Precison: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"FMeasure: {fMeasure}\")\n",
    "\n",
    "def alpha(X,Y,k):\n",
    "    yDiag = diag(Y)\n",
    "    ones = np.ones((X.shape[0],1))\n",
    "    return np.linalg.pinv(yDiag@k(X,X)@yDiag)@ones\n",
    "\n",
    "def k(a,b):\n",
    "    return a@b.T\n",
    "\n",
    "def svm(training,validation,headers,flds,c):\n",
    "    cols = []\n",
    "    for fld in flds:\n",
    "        cols.append(getFieldIndex(headers,fld))\n",
    "    cI = getFieldIndex(headers,c)\n",
    "    X = np.column_stack((np.ones((training.shape[0],1)),training[:,cols])).astype(float)\n",
    "    Y = np.where(training[:,cI] == 'TRUE',1,-1)\n",
    "    a = alpha(X,Y,k)\n",
    "    w = X.T@diag(Y)@a\n",
    "    yHat = X@w\n",
    "    print(\"Training\")\n",
    "    svmstats(yHat,Y)\n",
    "    print()\n",
    "    X = np.column_stack((np.ones((validation.shape[0],1)),validation[:,cols])).astype(float)\n",
    "    Y = np.where(validation[:,cI] == 'TRUE',1,-1)\n",
    "    yHat = X@w\n",
    "    print(\"Validation\")\n",
    "    svmstats(yHat,Y)\n",
    "    return np.where(yHat > 0,\"TRUE\",\"FALSE\")\n",
    "\n",
    "\n",
    "\n",
    "def kernel(a,b):\n",
    "    return ((a@b.T) + 1)**2\n",
    "\n",
    "def svmk(training,validation,headers,flds,c):\n",
    "    cols = []\n",
    "    for fld in flds:\n",
    "        cols.append(getFieldIndex(headers,fld))\n",
    "    cI = getFieldIndex(headers,c)\n",
    "    X = np.column_stack((np.ones((training.shape[0],1)),training[:,cols])).astype(float)\n",
    "    Y = np.where(training[:,cI] == 'TRUE',1,-1)\n",
    "    a = alpha(X,Y,kernel)\n",
    "    w = X.T@diag(Y)@a\n",
    "    yHat = kernel(X,X)@diag(Y)@a\n",
    "    print(\"Training\")\n",
    "    svmstats(yHat,Y)\n",
    "    print()\n",
    "    X2 = np.column_stack((np.ones((validation.shape[0],1)),validation[:,cols])).astype(float)\n",
    "    Y2 = np.where(validation[:,cI] == 'TRUE',1,-1)\n",
    "    yHat = kernel(X2,X)@diag(Y)@a\n",
    "    print(\"Validation\")\n",
    "    svmstats(yHat,Y2)\n",
    "    return np.where(yHat > 0,\"TRUE\",\"FALSE\")\n",
    "\n",
    "def logreg(training,validation,headers,flds,c):\n",
    "    #needs code filled in\n",
    "    cols = []\n",
    "    for fld in flds:\n",
    "        cols.append(getFieldIndex(headers, fld))\n",
    "    cI = getFieldIndex(headers, c)\n",
    "    X = np.column_stack((np.ones((training.shape[0], 1)), training[:, cols])).astype(float)\n",
    "    Y = np.where(training[:, cI] == 'TRUE', 1, 0)\n",
    "    w = np.zeros(X.shape[1])\n",
    "    logisticReg = 0.01\n",
    "    epochs = 780\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        linear_combination = X @ w\n",
    "        predictions = 1 / (1 + np.exp(-linear_combination))\n",
    "        error = predictions - Y\n",
    "        gradient = X.T @ error / Y.size\n",
    "        w -= logisticReg * gradient\n",
    "        if epoch % 100 == 0:\n",
    "            loss = -np.mean(Y * np.log(predictions) + (1 - Y) * np.log(1 - predictions))\n",
    "    validationX = np.column_stack((np.ones((validation.shape[0], 1)), validation[:, cols])).astype(float)\n",
    "    #validationY = np.where(validation[:, cI] == 'TRUE', 1, 0)\n",
    "    validationZ = validationX @ w\n",
    "    predictions = 1 / (1 + np.exp(-validationZ))\n",
    "    binPred = np.where(predictions > 0.5, \"TRUE\", \"FALSE\")\n",
    "    logAccuracy = np.mean(binPred == validation[:, cI])\n",
    "    print(f\"Logistic Regression: {logAccuracy}\")\n",
    "    \n",
    "    return binPred\n",
    "    #return np.full((validationData.shape[0],1),\"TRUE\")\n",
    "\n",
    "def linreg(training,validation,headers,flds,c):\n",
    "    cols = []\n",
    "    for fld in flds:\n",
    "        cols.append(getFieldIndex(headers,fld))\n",
    "    cI = getFieldIndex(headers,c)\n",
    "    X = np.column_stack((np.ones((training.shape[0],1)),training[:,cols])).astype(float)\n",
    "    Y = training[:,cI].astype(float)\n",
    "    w = np.linalg.pinv(X.T@X)@X.T@Y\n",
    "    train = X@w\n",
    "    print(\"Training LinReg:\")\n",
    "    linregstats(Y,train)\n",
    "    X = np.column_stack((np.ones((validation.shape[0],1)),validation[:,cols])).astype(float)\n",
    "    Y = validation[:,cI].astype(float)\n",
    "    yHat = X@w\n",
    "    print(\"Validation LinReg:\")\n",
    "    linregstats(Y,yHat)\n",
    "    return yHat\n",
    "\n",
    "def knn(trainingData,validationData,flds,c,K):\n",
    "    goalIndex = getFieldIndex(headers,c)\n",
    "    right = 0\n",
    "    results = []\n",
    "    for i in range(0,validationData.shape[0]):\n",
    "        kClosest = []\n",
    "        kClosestClass = []\n",
    "        for p in range(0,trainingData.shape[0]):\n",
    "            distance = 0\n",
    "            for fld in flds:\n",
    "                fldI = getFieldIndex(headers,fld)\n",
    "                distance += (validationData[i,fldI].astype(float) - trainingData[p,fldI].astype(float))**2\n",
    "            distance = distance ** 0.5\n",
    "            for k in range(0,K):\n",
    "                if len(kClosest) == k or kClosest[k] > distance:\n",
    "                    kClosest.insert(k,distance)\n",
    "                    kClosestClass.insert(k,trainingData[p,goalIndex])\n",
    "                    break\n",
    "            if len(kClosest) > K:\n",
    "                kClosest.pop()\n",
    "                kClosestClass.pop()\n",
    "        counter = Counter(kClosestClass)\n",
    "        predict = counter.most_common(1)[0][0]\n",
    "        results.append(predict)\n",
    "        confidence = counter[predict] / 5\n",
    "        actual = validationData[i,goalIndex]\n",
    "        if predict == actual:\n",
    "            right += 1\n",
    "    print(f\"Acuracy:{right/validationData.shape[0]}\")\n",
    "    return results\n",
    "\n",
    "#loading in data and splitting it\n",
    "csv = np.loadtxt(\"./data/games.csv\",dtype=str,delimiter=\",\")\n",
    "\n",
    "np.random.seed(0)\n",
    "headers = csv[0,:]\n",
    "home_score_col = getFieldIndex(headers,\"home_score\")\n",
    "away_score_col = getFieldIndex(headers,\"away_score\")\n",
    "data = csv[1:,:]\n",
    "data = data[data[:,home_score_col] != \"NA\"]\n",
    "data = data[data[:,away_score_col] != \"NA\"]\n",
    "\n",
    "oneHotFields = [\"weekday\",\"gametime\",\"roof\",\"surface\",\"stadium_id\"]\n",
    "oneHotFieldsAfter = {}\n",
    "for fld in oneHotFields:\n",
    "    col = getFieldIndex(headers,fld)\n",
    "    newHeaders,newData = onehot(data[:,col],fld)\n",
    "    headers = np.delete(headers,col)\n",
    "    headers = np.append(headers,newHeaders)\n",
    "    data = np.delete(data,col,axis=1)\n",
    "    data = np.hstack((data,newData))\n",
    "    oneHotFieldsAfter[fld] = newHeaders\n",
    "\n",
    "trainingData, validationData = trainingSplit(data)\n",
    "\n",
    "#zscore required fields\n",
    "zscoreFields = [\"home_rest\",\"away_rest\",\"pastYear_home_Offense_yards_gained\",\"past4_home_Offense_yards_gained\",\"pastYear_home_Offense_touchdown\",\"past4_home_Offense_touchdown\",\"pastYear_home_Offense_sack\",\"past4_home_Offense_sack\",\"pastYear_home_Offense_penalty_yards\",\"past4_home_Offense_penalty_yards\",\"pastYear_home_Offense_fumble_lost\",\"past4_home_Offense_fumble_lost\",\"pastYear_home_Offense_interception\",\"past4_home_Offense_interception\",\"pastYear_home_Defense_yards_gained\",\"past4_home_Defense_yards_gained\",\"pastYear_home_Defense_touchdown\",\"past4_home_Defense_touchdown\",\"pastYear_home_Defense_sack\",\"past4_home_Defense_sack\",\"pastYear_home_Defense_penalty_yards\",\"past4_home_Defense_penalty_yards\",\"pastYear_home_Defense_fumble_lost\",\"past4_home_Defense_fumble_lost\",\"pastYear_home_Defense_interception\",\"past4_home_Defense_interception\",\"pastYear_home_top\",\"past4_home_top\",\"pastYear_away_Offense_yards_gained\",\"past4_away_Offense_yards_gained\",\"pastYear_away_Offense_touchdown\",\"past4_away_Offense_touchdown\",\"pastYear_away_Offense_sack\",\"past4_away_Offense_sack\",\"pastYear_away_Offense_penalty_yards\",\"past4_away_Offense_penalty_yards\",\"pastYear_away_Offense_fumble_lost\",\"past4_away_Offense_fumble_lost\",\"pastYear_away_Offense_interception\",\"past4_away_Offense_interception\",\"pastYear_away_Defense_yards_gained\",\"past4_away_Defense_yards_gained\",\"pastYear_away_Defense_touchdown\",\"past4_away_Defense_touchdown\",\"pastYear_away_Defense_sack\",\"past4_away_Defense_sack\",\"pastYear_away_Defense_penalty_yards\",\"past4_away_Defense_penalty_yards\",\"pastYear_away_Defense_fumble_lost\",\"past4_away_Defense_fumble_lost\",\"pastYear_away_Defense_interception\",\"past4_away_Defense_interception\",\"pastYear_away_top\",\"past4_away_top\"]\n",
    "for fld in zscoreFields:\n",
    "    i = getFieldIndex(headers,fld)\n",
    "    trainingData[:,i],validationData[:,i] = zscore(trainingData[:,i],validationData[:,i])\n",
    "\n",
    "#run KNN based on only zscored fields\n",
    "#knn(trainingData,validationData,zscoreFields,\"home_win_spread\",10)\n",
    "\n",
    "#run LinReg\n",
    "validationData = np.column_stack((validationData,linreg(trainingData,validationData,headers,zscoreFields,\"home_score\")))\n",
    "validationData = np.column_stack((validationData,linreg(trainingData,validationData,headers,zscoreFields,\"away_score\")))\n",
    "hwI = getFieldIndex(headers,\"home_win_spread\")\n",
    "spreadI = getFieldIndex(headers,\"spread_line\")\n",
    "\n",
    "validationData = np.column_stack((validationData,validationData[:,-2].astype(float)-(validationData[:,-1].astype(float) + validationData[:,spreadI].astype(float))))\n",
    "validationData = np.column_stack((validationData,np.where(validationData[:,-1].astype(float) > 0,\"TRUE\",\"FALSE\")))\n",
    "print()\n",
    "#validationData = validationData[np.abs(validationData[:,-2].astype(float)) > 3]\n",
    "right = np.sum(validationData[:,-1] == validationData[:,hwI])\n",
    "print(f\"right:{right} size:{validationData.shape[0]} %:{right/validationData.shape[0]}\")\n",
    "for i in range(1,np.max(validationData[:,-2].astype(float)).astype(int)):\n",
    "    print(f\"i:{i}\")\n",
    "    d = validationData[np.abs(validationData[:,-2].astype(float)) > i]\n",
    "    right = np.sum(d[:,-1] == d[:,hwI])\n",
    "    print(f\"right:{right} size:{d.shape[0]} %:{right/d.shape[0]}\")\n",
    "\n",
    "print()\n",
    "#run SVM\n",
    "validationData = np.column_stack((validationData,svm(trainingData,validationData,headers,zscoreFields,\"home_win_spread\")))\n",
    "\n",
    "print()\n",
    "#run LogReg\n",
    "validationData = np.column_stack((validationData,logreg(trainingData,validationData,headers,zscoreFields,\"home_win_spread\")))\n",
    "\n",
    "#creates a result 2d array where \n",
    "#column 1 is the linear regression result\n",
    "#column 2 is the SVM result\n",
    "#column 3 is the logisitical regression result with \n",
    "#column 4 being the actual result \n",
    "results = np.column_stack((validationData[:,-3:],validationData[:,hwI]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Bagging Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BAGGING - Classification Accuracy: 0.5238095238095238\n",
      "BAGGING - Regression Mean Squared Error (MSE): 0.25172864597330463\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Convert 'TRUE'/'FALSE' to 1/0\n",
    "bg_results = np.where(results == 'TRUE', 1, 0).astype(int)\n",
    "\n",
    "X = bg_results[:, :3]  \n",
    "y_class = bg_results[:, 3]\n",
    "y_reg = bg_results[:, 3].astype(float)\n",
    "\n",
    "split_index = int(0.7 * len(X))  \n",
    "X_train, X_test = X[:split_index], X[split_index:]\n",
    "y_class_train, y_class_test = y_class[:split_index], y_class[split_index:]\n",
    "y_regr_train, y_regr_test = y_reg[:split_index], y_reg[split_index:]\n",
    "\n",
    "# Bagging for Classification\n",
    "num_models = 10\n",
    "classification_predictions = []\n",
    "\n",
    "for _ in range(num_models):\n",
    "    indices = np.random.choice(len(X_train), len(X_train), replace=True)\n",
    "    X_sample, y_sample = X_train[indices], y_class_train[indices]\n",
    "    \n",
    "    majority_class = Counter(y_sample).most_common(1)[0][0]\n",
    "    model_prediction = [majority_class] * len(X_test)\n",
    "    classification_predictions.append(model_prediction)\n",
    "\n",
    "final_classification_prediction = np.round(np.mean(classification_predictions, axis=0)).astype(int)\n",
    "classification_accuracy = np.mean(final_classification_prediction == y_class_test)\n",
    "\n",
    "regression_predictions = []\n",
    "\n",
    "for _ in range(num_models):\n",
    "    indices = np.random.choice(len(X_train), len(X_train), replace=True)\n",
    "    X_sample, y_sample = X_train[indices], y_regr_train[indices]\n",
    "    \n",
    "    model_prediction = [np.mean(y_sample)] * len(X_test)\n",
    "    regression_predictions.append(model_prediction)\n",
    "\n",
    "final_regression_prediction = np.mean(regression_predictions, axis=0)\n",
    "regression_mse = np.mean((final_regression_prediction - y_regr_test) ** 2)\n",
    "\n",
    "# Output results\n",
    "print(\"BAGGING - Classification Accuracy:\", classification_accuracy)\n",
    "print(\"BAGGING - Regression Mean Squared Error (MSE):\", regression_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RANDOM FOREST - Classification Accuracy: 0.5238095238095238\n",
      "RANDOM FOREST - Regression Mean Squared Error (MSE): 0.252138162104401\n"
     ]
    }
   ],
   "source": [
    "# Convert 'TRUE'/'FALSE' to 1/0\n",
    "rf_results = np.where(results == 'TRUE', 1, 0).astype(int)\n",
    "\n",
    "X = rf_results[:, :3]  \n",
    "y_class = rf_results[:, 3]  \n",
    "y_reg = rf_results[:, 3].astype(float) \n",
    "\n",
    "split_index = int(0.7 * len(X)) \n",
    "X_train, X_test = X[:split_index], X[split_index:]\n",
    "y_class_train, y_class_test = y_class[:split_index], y_class[split_index:]\n",
    "y_regr_train, y_regr_test = y_reg[:split_index], y_reg[split_index:]\n",
    "\n",
    "# Random Forests for Classification\n",
    "num_trees = 10\n",
    "max_features = 2 \n",
    "classification_predictions = []\n",
    "\n",
    "for _ in range(num_trees):\n",
    "    indices = np.random.choice(len(X_train), len(X_train), replace=True)\n",
    "    X_sample, y_sample = X_train[indices], y_class_train[indices]\n",
    "    \n",
    "    selected_features = np.random.choice(X_sample.shape[1], max_features, replace=False)\n",
    "    X_sample_reduced = X_sample[:, selected_features]\n",
    "    X_test_reduced = X_test[:, selected_features]\n",
    "    \n",
    "    majority_class = Counter(y_sample).most_common(1)[0][0]\n",
    "    model_prediction = [majority_class] * len(X_test_reduced)\n",
    "    classification_predictions.append(model_prediction)\n",
    "\n",
    "final_classification_prediction = np.round(np.mean(classification_predictions, axis=0)).astype(int)\n",
    "classification_accuracy = np.mean(final_classification_prediction == y_class_test)\n",
    "\n",
    "# Random Forests for Regression\n",
    "regression_predictions = []\n",
    "\n",
    "for _ in range(num_trees):\n",
    "    indices = np.random.choice(len(X_train), len(X_train), replace=True)\n",
    "    X_sample, y_sample = X_train[indices], y_regr_train[indices]\n",
    "    \n",
    "    selected_features = np.random.choice(X_sample.shape[1], max_features, replace=False)\n",
    "    X_sample_reduced = X_sample[:, selected_features]\n",
    "    X_test_reduced = X_test[:, selected_features]\n",
    "    \n",
    "    model_prediction = [np.mean(y_sample)] * len(X_test_reduced)\n",
    "    regression_predictions.append(model_prediction)\n",
    "\n",
    "final_regression_prediction = np.mean(regression_predictions, axis=0)\n",
    "regression_mse = np.mean((final_regression_prediction - y_regr_test) ** 2)\n",
    "\n",
    "# Output results\n",
    "print(\"RANDOM FOREST - Classification Accuracy:\", classification_accuracy)\n",
    "print(\"RANDOM FOREST - Regression Mean Squared Error (MSE):\", regression_mse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
